InputFile="Bath&NESomerset_dataset_airbnb-scraper-task-7_2023-02-17_12-40-40-928.json"
OutputFile="Bath&NESomerset_700plus.csv"
if (file.exists(paste0(OutputDir,OutputFile))){
Answer=readline(prompt="File already exists. Proceed? Y/N >")
if(Answer!="Y" & Answer !="y") {
stop("Aborting by request")}
}
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Read in the data scrape
JSON.Data <- fromJSON(paste0(InputDir,InputFile))
#glimpse(JSON.Data, max.level=1)
#glimpse(JSON.Data$pricing, max.level=1)
DF1=JSON.Data %>% select(numberOfGuests)
DF2=JSON.Data %>% select(roomType)
DF3=JSON.Data$pricing$rate %>% select(amount)
DF4=JSON.Data$pricing %>% select(rateType)
DF5=JSON.Data$primaryHost %>% select(firstName,memberSince,about)
DF6=JSON.Data %>% select(name,p3SummaryAddress)
DF7=JSON.Data$sectionedDescription %>% select(description,houseRules,
neighborhoodOverview,notes,
space,summary)
DF8=JSON.Data %>% select(additionalHouseRules)
DF9=data.frame(JSON.Data$guestControls$structuredHouseRules[[1]][1],
JSON.Data$guestControls$structuredHouseRules[[1]][2],
JSON.Data$guestControls$structuredHouseRules[[1]][3])
names(DF9)=c("Rule1","Rule2","Rule3")
DF10=JSON.Data$location %>% select(lat,lng)
DF11=JSON.Data %>% select(url,stars,isHostedBySuperhost,isHotel)
AllDF=as.data.frame(cbind(DF1,DF2,DF3,DF4,DF5,DF6,DF7,DF8,DF9,DF10,DF11))
nrow(AllDF)
AllDF = AllDF %>% filter(numberOfGuests>=8 & !is.na(numberOfGuests) &
numberOfGuests!="NA" & !is.na(amount) &
amount!="NA" & isHotel==FALSE &
roomType!="Campsite" & roomType!="Tent") %>%
arrange(firstName,name,p3SummaryAddress)
nrow(AllDF)
# Identify and remove duplicates
NewDF=AllDF[1,]
CurrentDF=NewDF
for (i in 2:nrow(AllDF)){
if(AllDF$firstName[i]!=CurrentDF$firstName[1] |
AllDF$name[i]!=CurrentDF$name[1] |
AllDF$p3SummaryAddress[i]!=CurrentDF$p3SummaryAddress[1]) {
NewDF=rbind(NewDF,AllDF[i,])
CurrentDF=AllDF[i,]
} else {
cat("Found internal duplicate = ",AllDF$url[i],"\n")}
}
# Can we check against any entries in previous files?
InFileList=list.files("./Output")
FileList=setdiff(InFileList,OutputFile)
Loops=length(FileList)
GotURLlist=data.frame(url=character())
for (i in 1:Loops){
Input=read.csv(paste0(OutputDir,FileList[i]))
Input = Input %>% select(url)
GotURLlist=rbind(GotURLlist,Input)
}
PropertyLoop=nrow(NewDF)
FinalDF=NewDF[0,]
for (i in 1:PropertyLoop){
if (!is.element(NewDF$url[i],GotURLlist$url)) {
FinalDF=rbind(FinalDF,NewDF[i,])
}else{
cat("Found external duplicate = ",NewDF$url[i],"\n")
}
}
nrow(FinalDF)
FinalDF = FinalDF %>% arrange(desc(numberOfGuests))
FinalDF = data.frame(lapply(FinalDF, function(x) {gsub("\n", " ", x)}))
write.csv(FinalDF,paste0(OutputDir,OutputFile),row.names = FALSE)
# End
#
# ConvertJSOBtoCSV.R
#
library(tidyverse)
library(rjson)
library(jsonlite)
library(plyr)
#library(tidyjson)
# Set & check Working Directory
setwd("H:/0000_Alex Personal/00_Ellen_Whisky/00_AirbnbSearch")
InputDir="./Input/"
OutputDir="./Output/"
getwd()
# Set input & output files <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
InputFile="Bath&NESomerset_300_699_dataset_airbnb-scraper-task-8_2023-02-17_14-52-35-748.json"
OutputFile="Bath&NESomerset_300_699.csv"
if (file.exists(paste0(OutputDir,OutputFile))){
Answer=readline(prompt="File already exists. Proceed? Y/N >")
if(Answer!="Y" & Answer !="y") {
stop("Aborting by request")}
}
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Read in the data scrape
JSON.Data <- fromJSON(paste0(InputDir,InputFile))
#glimpse(JSON.Data, max.level=1)
#glimpse(JSON.Data$pricing, max.level=1)
DF1=JSON.Data %>% select(numberOfGuests)
DF2=JSON.Data %>% select(roomType)
DF3=JSON.Data$pricing$rate %>% select(amount)
DF4=JSON.Data$pricing %>% select(rateType)
DF5=JSON.Data$primaryHost %>% select(firstName,memberSince,about)
DF6=JSON.Data %>% select(name,p3SummaryAddress)
DF7=JSON.Data$sectionedDescription %>% select(description,houseRules,
neighborhoodOverview,notes,
space,summary)
DF8=JSON.Data %>% select(additionalHouseRules)
DF9=data.frame(JSON.Data$guestControls$structuredHouseRules[[1]][1],
JSON.Data$guestControls$structuredHouseRules[[1]][2],
JSON.Data$guestControls$structuredHouseRules[[1]][3])
names(DF9)=c("Rule1","Rule2","Rule3")
DF10=JSON.Data$location %>% select(lat,lng)
DF11=JSON.Data %>% select(url,stars,isHostedBySuperhost,isHotel)
AllDF=as.data.frame(cbind(DF1,DF2,DF3,DF4,DF5,DF6,DF7,DF8,DF9,DF10,DF11))
nrow(AllDF)
AllDF = AllDF %>% filter(numberOfGuests>=8 & !is.na(numberOfGuests) &
numberOfGuests!="NA" & !is.na(amount) &
amount!="NA" & isHotel==FALSE &
roomType!="Campsite" & roomType!="Tent") %>%
arrange(firstName,name,p3SummaryAddress)
nrow(AllDF)
# Identify and remove duplicates
NewDF=AllDF[1,]
CurrentDF=NewDF
for (i in 2:nrow(AllDF)){
if(AllDF$firstName[i]!=CurrentDF$firstName[1] |
AllDF$name[i]!=CurrentDF$name[1] |
AllDF$p3SummaryAddress[i]!=CurrentDF$p3SummaryAddress[1]) {
NewDF=rbind(NewDF,AllDF[i,])
CurrentDF=AllDF[i,]
} else {
cat("Found internal duplicate = ",AllDF$url[i],"\n")}
}
# Can we check against any entries in previous files?
InFileList=list.files("./Output")
FileList=setdiff(InFileList,OutputFile)
Loops=length(FileList)
GotURLlist=data.frame(url=character())
for (i in 1:Loops){
Input=read.csv(paste0(OutputDir,FileList[i]))
Input = Input %>% select(url)
GotURLlist=rbind(GotURLlist,Input)
}
PropertyLoop=nrow(NewDF)
FinalDF=NewDF[0,]
for (i in 1:PropertyLoop){
if (!is.element(NewDF$url[i],GotURLlist$url)) {
FinalDF=rbind(FinalDF,NewDF[i,])
}else{
cat("Found external duplicate = ",NewDF$url[i],"\n")
}
}
nrow(FinalDF)
FinalDF = FinalDF %>% arrange(desc(numberOfGuests))
FinalDF = data.frame(lapply(FinalDF, function(x) {gsub("\n", " ", x)}))
write.csv(FinalDF,paste0(OutputDir,OutputFile),row.names = FALSE)
# End
#
# ExtractLatLong.R
#
#
# Prelim ----
#install.packages("rjson")
#install.packages("tidyjson")
#install.packages("plyr")
library(tidyverse)
#library(tidyjson)
# Set & check Working Directory
setwd("H:/0000_Alex Personal/00_Ellen_Whisky/00_AirbnbSearch/Output")
getwd()
FileList=list.files()
Loops=length(FileList)
Geography=data.frame(lat=numeric(),lng=numeric)
for (i in 1:Loops){
Input=read.csv(FileList[i])
Input = Input %>% select(lat,lng)
Geography=rbind(Geography,Input)
}
write.csv(Geography,"../Geography.csv",row.names=FALSE)
library(tidyverse)
library(rjson)
library(jsonlite)
library(plyr)
#library(tidyjson)
# Set & check Working Directory
setwd("H:/0000_Alex Personal/00_Ellen_Whisky/00_AirbnbSearch")
InputDir="./Input/"
OutputDir="./Output/"
getwd()
# Set input & output files <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
InputFile="Wiltshire_700plus_dataset_airbnb-scraper-task-9_2023-02-17_16-41-50-767.json"
OutputFile="Wiltshire_700plus.csv"
if (file.exists(paste0(OutputDir,OutputFile))){
Answer=readline(prompt="File already exists. Proceed? Y/N >")
if(Answer!="Y" & Answer !="y") {
stop("Aborting by request")}
}
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Read in the data scrape
JSON.Data <- fromJSON(paste0(InputDir,InputFile))
#glimpse(JSON.Data, max.level=1)
#glimpse(JSON.Data$pricing, max.level=1)
DF1=JSON.Data %>% select(numberOfGuests)
DF2=JSON.Data %>% select(roomType)
DF3=JSON.Data$pricing$rate %>% select(amount)
DF4=JSON.Data$pricing %>% select(rateType)
DF5=JSON.Data$primaryHost %>% select(firstName,memberSince,about)
DF6=JSON.Data %>% select(name,p3SummaryAddress)
DF7=JSON.Data$sectionedDescription %>% select(description,houseRules,
neighborhoodOverview,notes,
space,summary)
DF8=JSON.Data %>% select(additionalHouseRules)
DF9=data.frame(JSON.Data$guestControls$structuredHouseRules[[1]][1],
JSON.Data$guestControls$structuredHouseRules[[1]][2],
JSON.Data$guestControls$structuredHouseRules[[1]][3])
names(DF9)=c("Rule1","Rule2","Rule3")
DF10=JSON.Data$location %>% select(lat,lng)
DF11=JSON.Data %>% select(url,stars,isHostedBySuperhost,isHotel)
AllDF=as.data.frame(cbind(DF1,DF2,DF3,DF4,DF5,DF6,DF7,DF8,DF9,DF10,DF11))
nrow(AllDF)
AllDF = AllDF %>% filter(numberOfGuests>=8 & !is.na(numberOfGuests) &
numberOfGuests!="NA" & !is.na(amount) &
amount!="NA" & isHotel==FALSE &
roomType!="Campsite" & roomType!="Tent") %>%
arrange(firstName,name,p3SummaryAddress)
nrow(AllDF)
# Identify and remove duplicates
NewDF=AllDF[1,]
CurrentDF=NewDF
for (i in 2:nrow(AllDF)){
if(AllDF$firstName[i]!=CurrentDF$firstName[1] |
AllDF$name[i]!=CurrentDF$name[1] |
AllDF$p3SummaryAddress[i]!=CurrentDF$p3SummaryAddress[1]) {
NewDF=rbind(NewDF,AllDF[i,])
CurrentDF=AllDF[i,]
} else {
cat("Found internal duplicate = ",AllDF$url[i],"\n")}
}
# Can we check against any entries in previous files?
InFileList=list.files("./Output")
FileList=setdiff(InFileList,OutputFile)
Loops=length(FileList)
GotURLlist=data.frame(url=character())
for (i in 1:Loops){
Input=read.csv(paste0(OutputDir,FileList[i]))
Input = Input %>% select(url)
GotURLlist=rbind(GotURLlist,Input)
}
PropertyLoop=nrow(NewDF)
FinalDF=NewDF[0,]
for (i in 1:PropertyLoop){
if (!is.element(NewDF$url[i],GotURLlist$url)) {
FinalDF=rbind(FinalDF,NewDF[i,])
}else{
cat("Found external duplicate = ",NewDF$url[i],"\n")
}
}
nrow(FinalDF)
FinalDF = FinalDF %>% arrange(desc(numberOfGuests))
FinalDF = data.frame(lapply(FinalDF, function(x) {gsub("\n", " ", x)}))
write.csv(FinalDF,paste0(OutputDir,OutputFile),row.names = FALSE)
# End
library(tidyverse)
library(rjson)
library(jsonlite)
library(plyr)
#library(tidyjson)
# Set & check Working Directory
setwd("H:/0000_Alex Personal/00_Ellen_Whisky/00_AirbnbSearch")
InputDir="./Input/"
OutputDir="./Output/"
getwd()
# Set input & output files <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
InputFile="Wiltshire_300_699_dataset_airbnb-scraper-task-10_2023-02-17_17-09-51-299.json"
OutputFile="Wiltshire_300_699.csv"
if (file.exists(paste0(OutputDir,OutputFile))){
Answer=readline(prompt="File already exists. Proceed? Y/N >")
if(Answer!="Y" & Answer !="y") {
stop("Aborting by request")}
}
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Read in the data scrape
JSON.Data <- fromJSON(paste0(InputDir,InputFile))
#glimpse(JSON.Data, max.level=1)
#glimpse(JSON.Data$pricing, max.level=1)
DF1=JSON.Data %>% select(numberOfGuests)
DF2=JSON.Data %>% select(roomType)
DF3=JSON.Data$pricing$rate %>% select(amount)
DF4=JSON.Data$pricing %>% select(rateType)
DF5=JSON.Data$primaryHost %>% select(firstName,memberSince,about)
DF6=JSON.Data %>% select(name,p3SummaryAddress)
DF7=JSON.Data$sectionedDescription %>% select(description,houseRules,
neighborhoodOverview,notes,
space,summary)
DF8=JSON.Data %>% select(additionalHouseRules)
DF9=data.frame(JSON.Data$guestControls$structuredHouseRules[[1]][1],
JSON.Data$guestControls$structuredHouseRules[[1]][2],
JSON.Data$guestControls$structuredHouseRules[[1]][3])
names(DF9)=c("Rule1","Rule2","Rule3")
DF10=JSON.Data$location %>% select(lat,lng)
DF11=JSON.Data %>% select(url,stars,isHostedBySuperhost,isHotel)
AllDF=as.data.frame(cbind(DF1,DF2,DF3,DF4,DF5,DF6,DF7,DF8,DF9,DF10,DF11))
nrow(AllDF)
AllDF = AllDF %>% filter(numberOfGuests>=8 & !is.na(numberOfGuests) &
numberOfGuests!="NA" & !is.na(amount) &
amount!="NA" & isHotel==FALSE &
roomType!="Campsite" & roomType!="Tent") %>%
arrange(firstName,name,p3SummaryAddress)
nrow(AllDF)
# Identify and remove duplicates
NewDF=AllDF[1,]
CurrentDF=NewDF
for (i in 2:nrow(AllDF)){
if(AllDF$firstName[i]!=CurrentDF$firstName[1] |
AllDF$name[i]!=CurrentDF$name[1] |
AllDF$p3SummaryAddress[i]!=CurrentDF$p3SummaryAddress[1]) {
NewDF=rbind(NewDF,AllDF[i,])
CurrentDF=AllDF[i,]
} else {
cat("Found internal duplicate = ",AllDF$url[i],"\n")}
}
# Can we check against any entries in previous files?
InFileList=list.files("./Output")
FileList=setdiff(InFileList,OutputFile)
Loops=length(FileList)
GotURLlist=data.frame(url=character())
for (i in 1:Loops){
Input=read.csv(paste0(OutputDir,FileList[i]))
Input = Input %>% select(url)
GotURLlist=rbind(GotURLlist,Input)
}
PropertyLoop=nrow(NewDF)
FinalDF=NewDF[0,]
for (i in 1:PropertyLoop){
if (!is.element(NewDF$url[i],GotURLlist$url)) {
FinalDF=rbind(FinalDF,NewDF[i,])
}else{
cat("Found external duplicate = ",NewDF$url[i],"\n")
}
}
nrow(FinalDF)
FinalDF = FinalDF %>% arrange(desc(numberOfGuests))
FinalDF = data.frame(lapply(FinalDF, function(x) {gsub("\n", " ", x)}))
write.csv(FinalDF,paste0(OutputDir,OutputFile),row.names = FALSE)
# End
#
# ExtractLatLong.R
#
#
# Prelim ----
#install.packages("rjson")
#install.packages("tidyjson")
#install.packages("plyr")
library(tidyverse)
#library(tidyjson)
# Set & check Working Directory
setwd("H:/0000_Alex Personal/00_Ellen_Whisky/00_AirbnbSearch/Output")
getwd()
FileList=list.files()
Loops=length(FileList)
Geography=data.frame(lat=numeric(),lng=numeric)
for (i in 1:Loops){
Input=read.csv(FileList[i])
Input = Input %>% select(lat,lng)
Geography=rbind(Geography,Input)
}
write.csv(Geography,"../Geography.csv",row.names=FALSE)
### Preliminary ----
library(tidyverse)
library(janitor)  # easy way to add column total to dataframe using adorn_totals("row")
library(scales)   # for easy labelling of ggplots
library(patchwork)# for easy placement of plots side-by-side
#library(ggrepel)
# Define working directory
# Assumes script in /Rscripts directory - all paths relative
DataInputDir="../04_Data/processed/"
ExternalDataDir="../04_Data/external/"
DataOutputDir="../04_Data/processed/"
CSVOutputDir="../05_Outputs/CSVfiles/"
ForReportOutputDir="../05_Outputs/ForReport/"
PrivateDir="../06_Private/"
#----
# Extract summary stats reported in Report Section 2.1----
# Read in the full set of CCG invoices to summarise composition of the data
FullDataDF=readRDS(paste0(DataInputDir,"02_FullCollatedDataset.RDS"))
setwd("H:/00_Sheaff New Project/00_FinalReport/3SC/03_Rscripts)
DataInputDir="../04_Data/processed/"
# Define working directory
# Assumes script in /Rscripts directory - all paths relative
setwd("H:/00_Sheaff New Project/00_FinalReport/3SC/03_Rscripts")
DataInputDir="../04_Data/processed/"
ExternalDataDir="../04_Data/external/"
DataOutputDir="../04_Data/processed/"
CSVOutputDir="../05_Outputs/CSVfiles/"
ForReportOutputDir="../05_Outputs/ForReport/"
PrivateDir="../06_Private/"
#----
# Extract summary stats reported in Report Section 2.1----
# Read in the full set of CCG invoices to summarise composition of the data
FullDataDF=readRDS(paste0(DataInputDir,"02_FullCollatedDataset.RDS"))
nrow(FullDataDF) # 689,536
InvoicesExtractDF=readRDS(paste0(DataInputDir,"02_InvoicesExtractDF.RDS"))
nrow(InvoicesExtractDF) #
# Get 2018 pop data & join to VCSESpend Data
PopData=read.csv(paste0(ExternalDataDir,"CCG2018Pops.csv"))
FullDataDF %>% group_by(CCGName) %>% summarise(TotVCSESpend=sum(Spending)) %>%
arrange(TotVCSESpend) %>% mutate(Rank=seq(1:nrow(.)))
FullDataDF %>% group_by(CCGName) %>% summarise(TotVCSESpend=sum(Spending)) %>%
arrange(TotVCSESpend)
FullDataDF %>% group_by(CCGName)
FullDataDF %>% group_by(CCGName) %>% summarise(TotVCSESpend=sum(Spending))
names(FullDataDF)
Out = FullDataDF %>% group_by(CCGName) %>% summarise(TotVCSESpend=sum(Spending))
Out
FullDataDF %>% group_by(CCGName) %>% summarise(Count=n())
FullDataDF %>% group_by(CCGName) %>% summarise(Count=n(.))
#
### Preliminary ----
library(tidyverse)
library(janitor)  # easy way to add column total to dataframe using adorn_totals("row")
library(scales)   # for easy labelling of ggplots
library(patchwork)# for easy placement of plots side-by-side
FullDataDF %>% group_by(CCGName) %>% summarise(Count=n())
FullDataDF %>% filter(is.na(ExpenseArea))
CCGList=FullDataDF %>% group_by(CCGName) %>% summarise(TotalRecords=n())
# 02_AnalysisForReport.R
# Preamble ----
# This script implements all the analysis, including plot generation, used
# in the report 'CCG_VCSE_Spend_Report.docx' and github README.md
# It also produces the various summary .csv files in the github /outputs
# directory.
#----
### Preliminary ----
library(tidyverse)
library(janitor)  # easy way to add column total to dataframe using adorn_totals("row")
library(scales)   # for easy labelling of ggplots
library(patchwork)# for easy placement of plots side-by-side
#library(ggrepel)
# Define working directory
# Assumes script in /Rscripts directory - all paths relative
DataInputDir="../04_Data/processed/"
ExternalDataDir="../04_Data/external/"
DataOutputDir="../04_Data/processed/"
CSVOutputDir="../05_Outputs/CSVfiles/"
ForReportOutputDir="../05_Outputs/ForReport/"
PrivateDir="../06_Private/"
#----
# Extract summary stats reported in Report Section 2.1----
# Read in the full set of CCG invoices to summarise composition of the data
FullDataDF=readRDS(paste0(DataInputDir,"02_FullCollatedDataset.RDS"))
nrow(FullDataDF) # 689,536
# Read in the 'analytical dataset'
InvoicesExtractDF=readRDS(paste0(DataInputDir,"02_InvoicesExtractDF.RDS"))
# How many CCGs are covered?
length(unique(FullDataDF$CCGCode))         # 189 CCG codes in full dataset
length(unique(InvoicesExtractDF$CCGCode))  # 189 CCG name 'analytical dataset'
# Using the full dataset:
# How many 'bad' records?
BadRecords = FullDataDF %>% filter(is.na(Supplier) | Redacted==1 | PHB_CHC==1 | Uninformative==1)
BadRecords %>% summarise(n())
BadRecords %>% summarise(n())
### Preliminary ----
library(tidyverse)
library(janitor)  # easy way to add column total to dataframe using adorn_totals("row")
library(scales)   # for easy labelling of ggplots
library(patchwork)# for easy placement of plots side-by-side
DataInputDir="../04_Data/processed/"
ExternalDataDir="../04_Data/external/"
DataOutputDir="../04_Data/processed/"
CSVOutputDir="../05_Outputs/CSVfiles/"
ForReportOutputDir="../05_Outputs/ForReport/"
PrivateDir="../06_Private/"
#----
# Extract summary stats reported in Report Section 2.1----
# Read in the full set of CCG invoices to summarise composition of the data
FullDataDF=readRDS(paste0(DataInputDir,"02_FullCollatedDataset.RDS"))
nrow(FullDataDF) # 689,536
#
# Read in the 'analytical dataset'
InvoicesExtractDF=readRDS(paste0(DataInputDir,"02_InvoicesExtractDF.RDS"))
# How many CCGs are covered?
length(unique(FullDataDF$CCGCode))         # 189 CCG codes in full dataset
length(unique(InvoicesExtractDF$CCGCode))  # 189 CCG name 'analytical dataset'
# Using the full dataset:
# How many 'bad' records?
BadRecords = FullDataDF %>% filter(is.na(Supplier) | Redacted==1 | PHB_CHC==1 | Uninformative==1)
BadRecords %>% summarise(n())
library(dplyr)
BadRecords %>% summarise(n())
library(dplyr)
# Assumes script in /Rscripts directory - all paths relative
DataInputDir="../04_Data/processed/"
ExternalDataDir="../04_Data/external/"
DataOutputDir="../04_Data/processed/"
CSVOutputDir="../05_Outputs/CSVfiles/"
ForReportOutputDir="../05_Outputs/ForReport/"
PrivateDir="../06_Private/"
#----
# Extract summary stats reported in Report Section 2.1----
# Read in the full set of CCG invoices to summarise composition of the data
FullDataDF=readRDS(paste0(DataInputDir,"02_FullCollatedDataset.RDS"))
nrow(FullDataDF) # 689,536
# Read in the 'analytical dataset'
InvoicesExtractDF=readRDS(paste0(DataInputDir,"02_InvoicesExtractDF.RDS"))
# How many CCGs are covered?
length(unique(FullDataDF$CCGCode))         # 189 CCG codes in full dataset
length(unique(InvoicesExtractDF$CCGCode))  # 189 CCG name 'analytical dataset'
# Using the full dataset:
# How many 'bad' records?
BadRecords = FullDataDF %>% filter(is.na(Supplier) | Redacted==1 | PHB_CHC==1 | Uninformative==1)
BadRecords %>% summarise(n())
